provider: ollama
model: llama3
endpoint: http://localhost:11434
api_key: null
max_tokens: 4096
temperature: 0.7
